---
title: "TuneShift-KD: Knowledge Distillation and Transfer for Fine-tuned Models"
collection: publications
category: conferences
permalink: /publication/2025-06-01-tuneshift-kd
excerpt: 'A domain-specific distillation pipeline for transferring knowledge from fine-tuned LLMs to new foundation models.'
date: 2025-06-01
venue: 'ICLR 2026 (Under Review)'
paperurl: 'https://openreview.net/forum?id=VBFcEJOYV0'
citation: 'Guan, Y., **Ohene-Agyei, J.**, Kwan, D., Dandurand, J.S., Zhang, Y., Vijaykumar, N. (2025). <i>TuneShift-KD: Knowledge Distillation and Transfer for Fine-tuned Models.</i> NeurIPS 2025 (Under Review).'
---

This work introduces **TuneShift-KD**, a novel pipeline for data-free knowledge distillation in large language models. We propose a perplexity-based filtering technique that enables high-fidelity transfer across domain-specific and multilingual fine-tuned models â€” without access to original training data.
